{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Summary of POMDPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\n",
    "In MDP we assume that we can always observe the true state of environment and that the next state of the environment is only dependent on the current state and chosen action. With this assumtion, we could only take the current state to compute the next action and the result still be optimal. \n",
    "\n",
    "In the real world, such assumption often does not hold for applications in the real world. The robos always perceive the world through noisy sensors and can only see parts of the world through a camera, which could be seen as POMDPS.\n",
    "\n",
    "We need the history $h_t =(o_0,a_1,...a_t-1,o_t)$ as a vector containing all past observations and taken actions. It can be easily seen that this history is needed for the optiimal policy in POMDP since the single observations in each timestep do not hold the Markov property. For example we could have two different underlying states s1 and s2 whcih have the same observation o, but different next states with different next observations for the same action a. Therefore, to predict the next state and next observation it is not sufficient to use only o and a.\n",
    "\n",
    "There are two main ways to solve POMDPS, which transform a POMDP to an equivalent, but computational demanding MDP.\n",
    "\n",
    "- Information state: In this approach the full history ht is used as the state of the MDP, called information state. It can be easily seen that the full history fulfills the Markov property $P(h_t+1=(o_0,a_1,..,a_t,ot+1)|h_t,a_t)=P(o_t+1|h_t,a_t)$\n",
    "because all past histories and actions are already in the current history $h_t$ per definition. Hence, we have an informaition state MDP. But the information state space will be much larger than the original state space, since with each timestep the history grows exponentially and is also known as the curse of history.\n",
    "\n",
    "- Belief state: In this approach instead of remembering the whole history, which is not always feasible, a sufficient statistic of history is used. This statistic is the belief state, which represents the current belief of the agent in which current underlying state the agent is given the past observations and actions: $b_t(s_t)=p(s_t|o_{1:t},a_{1:t})$. For example if we have a POMDP with two discrete states s0 and s1, the belief state would be given by an one-dimentional vector between 0 and 1, where 0 corresponds to the belief that the world is in s0 and 1 corresponds to s1. For any other values between 0 and 1, 0<p<1 corresponds to the beliedf that we are with probability 1-p in s0 and probability p in s1. Using this belief state one can create a reward function and transition function based on beliedfs. Hence, we have a belief state MDP. But in this case we will have an MDP with continuous states and D-dimensional belief vector, where D=|S|. Furthermore, we need a method to update the current belief given a new observation and action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\n",
    "## Belief-based (tbc)\n",
    "We can see that a belief state b, which is a probability ditribution over S describing the probability of being at every state, such that $0\\leq b(s)\\leq 1$ and $\\sum{b(s)}=1$, one can create a belief MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
